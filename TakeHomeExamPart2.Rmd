---
title: "TakeHomeExamPart-2"
author: "Nittala Venkata Sai Aditya"
output: word_document
date: "2022-08-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1:Probability Practice 

**Part A**

P(yes)=0.65
P(No)=0.35

P(Rc)=0.3
P(TC)=0.7

P(Y|RC)=p(N|RC)=0.5

P(Yes) = P(RC)*P(Y|RC) + P(TC)*P(Y|TC)

0.65 = 0.3*0.5+0.7*P(Y|TC)
0.65 = 0.15+0.7*P(Y|TC)
0.5/0.7 = P(Y|TC)

P(Y|TC) =0.714 

* Probability of truthful clickers answering Yes is **0.714**


**Part B**

P(Positive |Disease) = 0.993
P(Negative | No Disease) = 0.9999

p(Disease) = 0.000025

* From Total Probability Rule

P(Positive) = P(Disease)* P(Positive|Disease)+P(Not Positive) * P(Positive | No Disease)

P(positive) = 0.000025 * 0.993 + 0.999975 * 0.0001
P(Positive) = 0.00002482 + 0.00009999
P(Positive) = 0.0001248

We have to find P(Disease | Positive)

* Going by Naive Bayes Theorem

P(disease | Positive) = (P(Positive|Disease)* P(Disease))/P(Positive)

P(disease | Positive) = (0.993*0.000025)/0.0001248

P(disease | Positive) ~ 0.2

* Final Probability = **0.2**

## Q2: Wrangling the Billboard Top 100

**Part A**

```{r 2a, echo=FALSE, message=FALSE,warning=FALSE}
library(readr)
library(dplyr)
billboard = read_csv('STA380/data/billboard.csv')
#str(billboard)
attach(billboard)
top10<- billboard %>% group_by(song,performer) %>% summarise(count=n()) %>% arrange(desc(count))
knitr::kable(head(top10,10),'simple',caption='Top 10 Popular Billboard songs')
  
```

**Part B**


```{r 2b, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="Musical Diversity Graph Per year.1960's was a great year for musical diversity"}
musicaldiversity <- billboard %>% filter(year!=1958 & year!=2021) %>% select(song,year) %>% group_by(year) %>% summarise(count=n_distinct(song)) %>% arrange(year)
library(ggplot2)
ggplot(musicaldiversity,aes(x=year,y=count)) + geom_line() + ylab('Unique Song Count')+ggtitle('Musical Diversity By Year')

```

**Part C**

```{r 2c, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="10 week hit songs count for Artists. Billy Joel has the most number of 10 week hit songs"}
artist <- top10 %>% filter(count>=10) %>% group_by(performer) %>% summarise(songs = n_distinct(song)) %>% filter(songs>=30) %>% arrange(desc(songs))
#artist
library(ggplot2)
ggplot(artist,aes(x=reorder(performer,-songs),y=songs)) + geom_bar(stat='identity') + coord_flip() + xlab('Artists') + ylab('10 weeks hits Count') + ggtitle('10 week hit songs for Artists')

```

## Q4. Visual story telling part 2: Capital Metro data

```{r 4a, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the monthly average of people boarding the buses during different times of the day"}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(ggthemes)
capMetro = read_csv('STA380/data/capmetro_UT.csv')

summary(capMetro)
#Let's summarize by hour for different months -
hour_board <- capMetro %>%
  group_by(month,hour_of_day) %>%
  summarize(avg_board = mean(boarding))

plt_1 = ggplot(hour_board, aes(x=hour_of_day, y=avg_board,color=month))+
  geom_line()+
  theme_stata()+
  ggtitle("Mean average of people boarding buses per month")+
  xlab("Hour of Day")+
  ylab("No. of people on-boarding")

#Let's summarize by hour for different months -
hour_alit <- capMetro %>%
  group_by(month,hour_of_day) %>%
  summarize(avg_alit = mean(alighting))


plt_2 = ggplot(hour_alit, aes(x=hour_of_day, y=avg_alit,color=month))+
  geom_line()+
  theme_stata()+
  ggtitle("Mean average of people off boarding buses per month")+
  xlab("Hour of Day")+
  ylab("No. of people off-boarding")

par(mfrow=c(1,2))
plt_1


```

```{r 4b, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the monthly average of people off boarding the buses during different times of the day"}
plt_2
```



We can make the following observations -


* The no. of people on-boarding the bus peaks around 4-6pm in the evening which is when most classes get over and students are heading home.

* No. of people off-boarding the bus is highest in the morning hours, possibly when most students get off at campus for their morning lectures.

* The distribution for no. of people on-boarding and off-boarding doesn't change much in any month


* The graphs are not in sync (spikes in on-boarding don't coincide with spikes in off-boarding). This may be the case because students all get off at the same location in campus together but they board the bus over a span of couple of hours (so average boarding per hour is low but off-boarding per hour is high) with the same logic being applied to spikes in on-boarding count.


* Average ridership is the least in the month of November (maybe because it's too cold and students don't want to take public transport) and most in the month of October


This distribution seems to be heavily influenced by students going to and from campus for college. Let's try to look at the distribution on the weekends -

```{r 4c, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the weekly average of people boarding the buses during different times of the day on weekends"}
hour_board_day <- capMetro[capMetro$weekend=="weekend",] %>%
  group_by(month,hour_of_day) %>%
  summarize(avg_board = mean(boarding))

plt_3 = ggplot(hour_board_day, aes(x=hour_of_day, y=avg_board,color=month))+
  geom_line()+
  theme_stata()+
  ggtitle("Weekend variation in bording buses")+
  xlab("Hour of Day")+
  ylab("No. of people on-boarding")

#Let's summarize by hour for different months -
hour_alit_day <- capMetro[capMetro$weekend=="weekend",] %>%
  group_by(month,hour_of_day) %>%
  summarize(avg_alit = mean(alighting))


plt_4 = ggplot(hour_alit_day, aes(x=hour_of_day, y=avg_alit,color=month))+
  geom_line()+
  theme_stata()+
  ggtitle("Weekend variation in off bording buses")+
  xlab("Hour of Day")+
  ylab("No. of people off-boarding")

par(mfrow=c(1,2))
plt_3

```


```{r 4d, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the weekly average of people off boarding the buses during different times of the day on weekends"}

plt_4
```


* The counts are much more varying throughout the day now

* There is an interesting spike in no. of off-boarding people in October towards the end of the day. Maybe this is because October is when students have their mid-terms for the semester so they tend to stay late on campus and go home during late hours of the day.


Let's also look at how weekend ridership changes based on temperature (since the weekday ridership is expected to not be affected by temperature since students have to go to college regardless) -

```{r 4e, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the average of people boarding the buses during different times of the day based on the temperature"}
# Create temperature variable -
capMetro$temp_bucket = unlist(lapply(capMetro$temperature, function(x){if (x > 29 & x<60){"29-60"} 
  else if(x>=60 & x<80){"60-80"} else if (x>=80 & x<100){"80-100"} }))

# Group by this new variable -
hour_board_temp <- capMetro[!is.na(capMetro$temperature)&capMetro$weekend=="weekend",] %>%
  group_by(temp_bucket,hour_of_day) %>%
  summarize(avg_board = mean(boarding))

plt_5 = ggplot(hour_board_temp, aes(x=hour_of_day, y=avg_board,color=temp_bucket))+
  geom_line()+
  theme_stata()+
  ggtitle("Temperature variation in boarding buses")+
  xlab("Hour of Day")+
  ylab("No. of people on-boarding")

#Let's summarize by hour for different months -
hour_alit_temp <- capMetro[!is.na(capMetro$temperature)&capMetro$weekend=="weekend",] %>%
  group_by(temp_bucket,hour_of_day) %>%
  summarize(avg_alit = mean(alighting))


plt_6 = ggplot(hour_alit_temp, aes(x=hour_of_day, y=avg_alit,color=temp_bucket))+
  geom_line()+
  theme_stata()+
  ggtitle("Temperature variation in off boarding buses")+
  xlab("Hour of Day")+
  ylab("No. of people off-boarding")

par(mfrow=c(1,2))
plt_5


```


```{r 4f, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the average of people off boarding the buses during different times of the day based on the temperature"}
plt_6
```


* The temperature doesn't seem to affect ridership during the weekend so much since the patterns and numbers match those when we don't account for temperature separately.
We've got some pretty interesting insights from these graphs!

Finally, let's look at the difference in total on-boardings and off-boardings per day -

```{r 4g, echo=FALSE, message=FALSE,warning=FALSE,fig.cap="A look at the differnet in the off boarders and on boarder during this period"}
capMetro$day = format(as.Date(capMetro$timestamp,format="%Y-%m-%d %H:%M:%S"), format = "%Y-%m-%d")
day_sum_diff <- capMetro %>%
  group_by(day) %>%
  summarize(sum_board=sum(boarding), sum_alit = sum(alighting))
day_sum_diff$diff = day_sum_diff$sum_board - day_sum_diff$sum_alit

plt_7 = ggplot(day_sum_diff, aes(x=day, y=diff))+
  geom_point()+
  geom_line(aes(x=day, y=diff))+
  theme_stata()+
  ggtitle("Difference in on-boarders and off-boarders")+
  xlab("Date")+
  ylab("No. of on-boarders - No. of off-boarders")+
  scale_x_discrete(breaks=day_sum_diff$day[c(T,F,F,F,F)])+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
plt_7

```

* We can see that there is a huge discrepancy between no. of on-boarders and no. of off-boarders every day (ideally the difference should be 0 unless ~300 people are hiding in the bus at the end of each day). Capital Metro needs to work on the optical metro system a bit more to get an accurate count!

## Q5: Portfolio Modelling

We want to pick a diversified portfolio which can give all the different combinations of ETFs and you try to get the maximum return possible.

**Portfolio -1**

**Scenario-1: Assign equal weights**

* EQUITY ETFS

We want to experiment with picking Equity ETF first and seeing if they have any impact on the VAR. The idea is to analyse if just picking different kinds of large cap/mid cap or small cap funds in the Equity asset class has any bearing on the final Var.
The first portfolio consists of 3 large cap, 2 medium cap and 2 small cap and 1 multi cap equity ETF's. The aim is to see how the size of the assets have an impact on the overall VAR.

The following ETF's are part of portfolio - 1

* Vanguard (VOO) : This ETF tracks the S&P 500 Index, one of the most famous benchmarks in the world and one that tracks some of America’s largest companies. VOO is more diversified than most, containing just over 500 securities in total. As a result, this fund could serve as a building block for many portfolios making it an excellent choice for many buy and holders, especially for those looking to keep costs at a minimum. It also has a low expense ratio of 0.03%. It has a mix of technology, financial and electronic sector as well. Asset Cap - 273,793 millions

* Invesco QQQ Trust (QQQ) : This ETF offers exposure to one of the world’s most widely-followed equity benchmarks, the NASDAQ, and has become one of the most popular exchange-traded products. The significant average daily trading volumes reflect that QQQ is widely used as a trading vehicle, and less as a components of a balanced long-term strategy. This also is generally in the technological sector.
Asset Cap - 177,912 millions

* Shares Core S&P 500 ETF (IVV) : IVV has become one of the largest ETFs in the world, offering exposure to one of the world’s best-known and most widely followed stock indexes. This ETF tracks the S&P 500 Index, which includes many large and well known U.S. firms. It has a high P/E ratio and dividend along with a low expense ratio. Asset Cap - 308,926 millions

* iShares Core S&P Mid-Cap ETF (IJH) : This ETF is one of several ETFs available that offers exposure to mid cap U.S. stocks, an asset class that can make up a significant portion of long-term, buy-and-hold portfolios.The expense ratio is competitive with the other options out there. Finance and Producer Management are the more prevalent sectors

* Vanguard Mid-Cap ETF (VO) : VO offers exposure to a balanced portfolio of stocks, including close to 460 individual names and spreading exposure relatively evenly. The expense ratio is among the cheapest in the category making it an excellent choice for those looking to keep costs to an absolute minimum.

* Vanguard Small Cap Value ETF (VBR) : VBR seeks to replicate a benchmark which offers exposure small cap firms that exhibit value characteristics in the U.S. equity market. The investment thesis behind small caps is that these firms are likely to provide strong growth prospects to a portfolio and should have a much easier time growing then their large cap counterparts. 

* iShares Core S&P Small-Cap ETF (IJR) : This ETF is linked to an index which tracks the performance of small cap U.S. stocks. This fund will make for a good investment for traders looking for growth and are aware of the risks that come along with investing in a small cap ETF.

* SPDR S&P Dividend ETF (SDY) : This ETF is linked to the S&P High Yield Dividend Aristocrats Index, which offers exposure to dividend paying large-cap companies that exhibit value characteristics within the U.S. equity market.Only the highest yielding companies are chosen and these firms must have increased dividends every year for at least 25 consecutive years. Thanks to this focus, SDY only invests in companies that are most likely to continue to pay out dividends in the future making it a solid pick for dividend focused investors even if the diversification is a little lacking.


```{r 3a1, echo=FALSE, message=FALSE,warning=FALSE}
library(mosaic)
library(quantmod)
library(foreach)


stockList_p1 = c('QQQ','IVV','VOO','IJH','VO','VBR','SDY','IJR')

#Extract the data from the database
getSymbols(stockList_p1,from='2017-01-01')

for(ticker in stockList_p1) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
#Get Percentage close to close from previous day

all_returns = cbind(ClCl(QQQa),ClCl(IVVa),ClCl(VOOa),ClCl(IJHa),ClCl(VOa),ClCl(VBRa),ClCl(SDYa),ClCl(IJRa))

#,
all_returns = as.matrix(na.omit(all_returns))
#Remove the first column since it's current day - previous day. As a result we get na

pairs(all_returns)

N = nrow(all_returns)

```

* Most of the ETFs seem to be correlated since most of them belong to the same sectors where Technology is the main contributor.
* Most ETFs are not closely correlated with IJR since IJR has more of a financial sector background and also is a small cap.

```{r 3a2, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
plot(all_returns[1:(N-1),3], all_returns[2:N,3],xlab='Current Day Return',ylab='Previous Day Return',main = 'Current vs Previous Returns Relation for Vooa')
```
* The above plot shows that there is no correlation between current and yesterday's returns for a sample ETF which is on expected lines otherwise it would have been easy to predict and exploit the markets


```{r 3a3, echo=FALSE, message=FALSE,warning=FALSE}
 set.seed(1)
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125)
	n_days = 20
	wealthtracker_p1 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker_p1[today] = total_wealth
	}
	wealthtracker_p1
}

hist(sim1[,n_days], 25,main = 'Histogram of returns for past 20 days for P1S1')


```
* The mean of the returns is plotted above and it can be seen that the mean is slightly beyond 100000 which was our initial amount

```{r 3a4, echo=FALSE, message=FALSE,warning=FALSE}
plot(wealthtracker_p1, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P1S1')
```
* The above plot shows the fluctuation in the wealth in the 20 day trading window. We see a small profit in all of the days and not a loss so that's a good measure of our portfolio.

```{r 3a5, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p1 = mean(sim1[,n_days] - initial_wealth)
cat('Mean of profits for P1S1is: ')
meanReturn_p1

```


```{r 3a6, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p1 = quantile(sim1[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 1 and scenario 1 is : ")
var_p1
```
```{r 3a7, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p1= sim1[,n_days]- initial_wealth
(hist(profit_p1,breaks = 30, main='Histogram of profits/Losses for P1S1',xlab="Profit"))
abline(v=var_p1,col="red",lwd=3)
abline(v=mean(profit_p1),col="blue",lwd=3)
text(-14700,400,"VAR Risk Line")
text(6500,700,"Mean Line")
```

* There's a 0.05 probability that we will lose the mention VAR score over the course of this 20 day window for our portfolio.

* Let's calculate the mean returns for all the columns and try to assign more weight to the higher returnees and lower weights to the lower returnee ETF's

* Let's look at the histogram distribution of the ETF's returns. 

```{r 3a8, echo=FALSE, message=FALSE,warning=FALSE}
all_returnsDf_p1 = as.data.frame(all_returns)
meanDf_p1 =  all_returnsDf_p1 %>% summarise_if(is.numeric, mean)
library(Hmisc)
hist.data.frame(all_returnsDf_p1)
```

* IJR has more returns followed by QQQ so we will assign more weights to these 2 and assign the least to VBR and IJH

```{r 3a9, echo=FALSE, message=FALSE,warning=FALSE}
unlistedDf_p1 = data.frame(score = unlist(meanDf_p1))

rowList_p1 = rownames(unlistedDf_p1)
newDf_p1 = as.data.frame(cbind(rowList_p1,unlistedDf_p1))
ggplot(newDf_p1,
            aes(x=rowList_p1,y=score)) +
 geom_bar(stat="identity") + ggtitle("ETF Returns for Portfolio -1") +
 xlab("ETF") + ylab("ETF Return Score")  

```

**Scenario-2: Assigning more weight to highe return ETF**

* Assign half weight to IJR and 0.2 to QQQ and least weight of 0.03 to VBR and IJH

```{r 3a10, echo=FALSE, message=FALSE,warning=FALSE}
set.seed(1)
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.200,0.06,0.06,0.03,0.06,0.03,0.06,0.5)
	n_days = 20
	wealthtracker_p1s2 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker_p1s2[today] = total_wealth
	}
	wealthtracker_p1s2
}

hist(sim1[,n_days], 25,main = 'Histogram of returns for past 20 days for P1S2')
```

```{r 3a11, echo=FALSE, message=FALSE,warning=FALSE}
plot(wealthtracker_p1s2, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P1S2')
```
* Mean of profits is more when we assign more weight to IJH. This could be due to the presence of more financial sector stocks and we recently had a boom coming out of covid for these stocks. As a result the returns are more for those ETF

```{r 3a12, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p1s2 = mean(sim1[,n_days] - initial_wealth)
cat('Mean of profits for P1S2 is: ')
meanReturn_p1s2
```
```{r 3a13, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p1s2 = quantile(sim1[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 1 and scenario 2 is : ")
var_p1s2
```

```{r 3a14, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p1s2= sim1[,n_days]- initial_wealth
(hist(profit_p1s2,breaks = 30, main='Histogram of profits/Losses for P1S2',xlab="Profit"))
abline(v=var_p1s2,col="red",lwd=3)
abline(v=mean(profit_p1s2),col="blue",lwd=3)
text(-14700,400,"VAR Risk Line")
text(6500,700,"Mean Line")
```
* Value of risk is more than the previous case since we bet more aggressively on one ETF.

**Portfolio-2**

**Scenario-1 : Distribute all funds equally**

* We will now create a portfolio with a mixture of Equity and Bond ETFs so that there is a bit more diversity in the asset classes.We will take 4 bond and 4 equity ETFs from the previous portfolio list

* Vanguard Total Bond Market ETF (BND) : This popular ETF offers exposure to entire investment grade bond market in a single ticker, with holdings in T-Bills, corporates, MBS, and agency bonds. This has the largest market capital and has a small expense ratio as well.

* iShares Core U.S. Aggregate Bond ETF (AGG) : The largest exchange-traded bond fund out there and one of the top 10 ETFs in the U.S. by assets, AGG boasts roughly $81 billion under management and is the simplest way to gain exposure to fixed-income markets. It is made up of more than 10,000 individual bond holdings to represent broad exposure to U.S. investment-grade bonds, including about 40% of its portfolio in U.S. Treasury bonds. The rest includes top-tier corporate bonds from firms like JPMorgan Chase & Co. (JPM) as well as mortgage-related debt. With loe expense ratio, AGG is a very affordable one-stop shop for bond exposure.

* Vanguard Short-Term Corporate Bond ETF (VCSH) : It is a $41 billion fund that focuses on high-quality corporate debt but with the typical bond in its portfolio maturing in just 2.8 years. That means investors can have a lot more certainty that those debts will be repaid in full, since it's a smaller window of time for unexpected disruptions to upend operations at these firms. The yield is a bit less than the longer-dated VCIT but is still very attractive when compared with the typical S&P 500 dividend stock – and offering a lot less risk, which is perhaps a big selling point all by itself in the current environment.

* iShares 20+ Year Treasury Bond ETF (TLT) : If you don't want to diversify into corporate debt and instead want the rock-solid comfort of the U.S. Treasury alone, perhaps the most popular low-risk way to play the bond market is TLT.The duration of this bond ETF's holdings are all 20 years or longer, which does provide some long-term interest rate risk; the fund is actually down 20% in the last 12 months as rates have moved higher and devalued its older positions. However, with a yield that is now roughly 60% higher than what it was just a year ago, it might be time to consider carving out a position once more in this $19 billion low-risk bond fund.

We will choose IJR, QQQ, VOO and VO from the above equity ETF's

```{r 3b1, echo=FALSE, message=FALSE,warning=FALSE}
library(mosaic)
library(quantmod)
library(foreach)


stockList_p2 = c('QQQ','BND','VOO','AGG','VO','VCSH','TLT','IJR')

#Extract the data from the database
getSymbols(stockList_p2,from='2017-01-01')

for(ticker in stockList_p2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
#Get Percentage close to close from previous day

all_returns_p2 = cbind(ClCl(QQQa),ClCl(BNDa),ClCl(VOOa),ClCl(AGGa),ClCl(VOa),ClCl(VCSHa),ClCl(TLTa),ClCl(IJRa))

#,
all_returns_p2 = as.matrix(na.omit(all_returns_p2))
#Remove the first column since it's current day - previous day. As a result we get na

pairs(all_returns_p2)

N = nrow(all_returns_p2)
```

* The Equity ETFs are not correlated with the bond ETFs as seen in the graph since its a different asset class itself. 
* The Bond ETFs are not that correlated with themselves as well since bonds are individual stocks with no sector as such and as a result the correlation is not prevalent

```{r 3b2, echo=FALSE, message=FALSE,warning=FALSE}
 set.seed(1)
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth_p2 = initial_wealth
	weights = c(0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125)
	n_days = 20
	wealthtracker_p2 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_p2, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth_p2
		holdings = holdings + holdings*return.today
		total_wealth_p2 = sum(holdings)
		wealthtracker_p2[today] = total_wealth_p2
	}
	wealthtracker_p2
}

hist(sim2[,n_days], 25,main = 'Histogram of returns for past 20 days for P2S1')

```
* The returns are in the slightly positive side with normal distribution more towards the right side 

```{r 3b3, echo=FALSE, message=FALSE,warning=FALSE}
plot(wealthtracker_p2, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P2S1')
```
* Max returns were reached around day 18

```{r 3b4, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p2 = mean(sim2[,n_days] - initial_wealth)
cat('Mean of profits for P2S1 is: ')
meanReturn_p2
```


```{r 3b5, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p2_1 = quantile(sim2[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 2 and scenario 1 is : ")
var_p2_1
```

* There is lesser value at risk and lesser mean returns than portfolio 1. This is because of the presence of both equity and bond ETFs The higher volatility of stocks relative to bonds is due to the nature of the two types of investments. When you buy stocks, you’re buying ownership in companies (albeit a small share). When you buy bonds, you’re lending money, either to companies or to governments. Because creditors are paid before owners, it’s riskier to own a company than it is to lend money, so the prices of stocks are more sensitive to changes in the economy.Thus bonds have lesser var but lesser returns 


```{r 3b6, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p2= sim2[,n_days]- initial_wealth
(hist(profit_p2,breaks = 30, main='Histogram of profits/Losses for P2S1',xlab="Profit"))
abline(v=var_p2_1,col="red",lwd=3)
abline(v=mean(profit_p2),col="blue",lwd=3)
text(8000,400,"Mean Line")
text(-10500,700,"Var risk Line")

```


**Scenario-2: Assign more weights to high return ETFs**


```{r 3b7, echo=FALSE, message=FALSE,warning=FALSE}
all_returns_p2Df = as.data.frame(all_returns_p2)
meanDf_p2 =  all_returns_p2Df %>% summarise_if(is.numeric, mean)
library(Hmisc)
hist.data.frame(all_returns_p2Df)
```

```{r 3b8, echo=FALSE, message=FALSE,warning=FALSE}
unlistedDf_p2 = data.frame(score = unlist(meanDf_p2))

rowList_p2 = rownames(unlistedDf_p2)
newDf_p2 = as.data.frame(cbind(rowList_p2,unlistedDf_p2))
ggplot(newDf_p2,
            aes(x=rowList_p2,y=score)) +
 geom_bar(stat="identity") + ggtitle("ETF Returns for Portfolio -2") +
 xlab("ETF") + ylab("ETF Return Score") 
```
* IJR,  QQQ, VOO are the better ETFs so we will assign more weights to them and lesser weight to BND and Agg

```{r 3b9, echo=FALSE, message=FALSE,warning=FALSE}
 set.seed(1)
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth_p2s2 = initial_wealth
	weights = c(0.2,0.02,0.2,0.03,0.05,0.05,0.05,0.4)
	n_days = 20
	wealthtracker_p2s2 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_p2, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth_p2s2
		holdings = holdings + holdings*return.today
		total_wealth_p2s2 = sum(holdings)
		wealthtracker_p2s2[today] = total_wealth_p2s2
	}
	wealthtracker_p2s2
}

hist(sim2[,n_days], 25,main = 'Histogram of returns for past 20 days for P2S2')
```

```{r 3b10, echo=FALSE, message=FALSE,warning=FALSE}

plot(wealthtracker_p2s2, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P2S2')
```

```{r 3b11, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p2s2 = mean(sim2[,n_days] - initial_wealth)
cat('Mean of profits for P2S2 is: ')
meanReturn_p2s2
```

```{r 3b12, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p2_s2 = quantile(sim2[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 2 and scenario 2 is : ")
var_p2_s2
```

* Theres not much of a difference in mean return and var but scenario 2 tends to perform better than scenario - 1.

```{r 3b13, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p2s2= sim2[,n_days]- initial_wealth
(hist(profit_p2s2,breaks = 30, main='Histogram of profits/Losses for P2S2',xlab="Profit"))
abline(v=var_p2_s2,col="red",lwd=3)
abline(v=mean(profit_p2s2),col="blue",lwd=3)
text(8000,800,"MeanLine")
text(-15500,700,"VAR Risk Line")

```
* The returns have increased but the var has gone down and the returns are still less than portfolio - 1

**Portfolio-3**

* We will now go for mix of equity, bonds, commodity,real estate and some currency ETF's

* Invesco DB US Dollar Index Bullish Fund (UUP) : This ETF offers exposure to a basket of currencies relative to the U.S. dollar, decreasing in value when the trade-weighted basket strengthens and increasing when the dollar appreciates. It is appropriate for investors seeking to a fund that is inversely correlated to the broad stock market or for those making a bet on a flight to quality so the aim is that it will help offset some losses

* SPDR Gold Shares (GLD) : GLD is one of the most popular ETFs in the world, offering exposure to an asset class that has become increasingly important to the asset allocation process in recent years. GLD can be used in a number of different ways; some may establish short term positions as a way of hedging against equity market volatility, dollar weakness, or inflation. Others may wish to include gold exposure as part of a long-term investment strategy. GLD is a relatively straightforward product; the underlying assets consist of gold bullion stored in secure vaults. As such, the price of this ETF can be expected to move in lock step with spot gold prices. 

* Vanguard Real Estate ETF (VNQ) : The Vanguard Real Estate Trust (VNQ) offers broad exposure to U.S. equity REITs, alongside a small allocation to specialized REITs and real estate firms. Real estate has historically been embraced because of its ability to deliver excess returns during bull markets and for its low correlation with traditional stock and bond investments. REITs might appeal to investors seeking current income, as these trusts must distribute at least 90% of their income to investors. The fund offers an efficient way for investors to gain indirect exposure to real estate prices

* iShares Gold Trust (IAU) : This fund offers exposure to one of the world’s most famous metals, gold. IAU is designed to track the spot price of gold bullion by holding gold bars in a secure vault, allowing investors to free themselves from finding a place to store the metal. While IAU isn’t the most liquid way to gain exposure to gold, it does have among the lowest expense ratios, making it a solid choice for cost-conscious investors.

We will take 2 equity (IJR,QQQ)  and 2 bond ETFS (TLT,Agg) from the above portfolios


```{r 3c1, echo=FALSE, message=FALSE,warning=FALSE}
library(mosaic)
library(quantmod)
library(foreach)


stockList_p3 = c('QQQ','UUP','GLD','AGG','VO','VNQ','IAU','IJR')

#Extract the data from the database
getSymbols(stockList_p3,from='2017-01-01')

for(ticker in stockList_p3) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
#Get Percentage close to close from previous day

all_returns_p3 = cbind(ClCl(QQQa),ClCl(UUPa),ClCl(GLDa),ClCl(AGGa),ClCl(VOa),ClCl(VNQa),ClCl(IAUa),ClCl(IJRa))

#,
all_returns_p3 = as.matrix(na.omit(all_returns_p3))
#Remove the first column since it's current day - previous day. As a result we get na

pairs(all_returns_p3)

N = nrow(all_returns_p3)
```

* Due to the different asset classes, there is not much correlation between the ETF's

```{r 3c2, echo=FALSE, message=FALSE,warning=FALSE}
 set.seed(1)
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth_p3 = initial_wealth
	weights = c(0.125,0.125,0.125,0.125,0.125,0.125,0.125,0.125)
	n_days = 20
	wealthtracker_p3 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_p3, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth_p3
		holdings = holdings + holdings*return.today
		total_wealth_p3 = sum(holdings)
		wealthtracker_p3[today] = total_wealth_p3
	}
	wealthtracker_p3
}

hist(sim3[,n_days], 25,main = 'Histogram of returns for past 20 days for P3S1',xlab='Holdings')

```

```{r 3c3, echo=FALSE, message=FALSE,warning=FALSE}
plot(wealthtracker_p3, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P3S1')
```

```{r 3c4, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p3s1 = mean(sim3[,n_days] - initial_wealth)
cat('Mean of profits for P3S1 is: ')
meanReturn_p3s1
```

```{r 3c5, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p3_s1 = quantile(sim3[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 3 and scenario 1 is : ")
var_p3_s1
```

```{r 3c6, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p3s1= sim3[,n_days]- initial_wealth
(hist(profit_p3s1,breaks = 30, main='Histogram of profits/Losses for P3S1',xlab="Profit"))
abline(v=var_p3_s1,col="red",lwd=3)
abline(v=mean(profit_p3s1),col="blue",lwd=3)
text(6000,800,"MeanLine")
text(-10500,700,"VAR Risk Line")

```

* The returns are slightly better than portfolio-2 but still less than portfolio - 1 since more returns are obtained for equity ETF's


**Scenario-2 : Assign weights to high performing ETF**

```{r 3c7, echo=FALSE, message=FALSE,warning=FALSE}
all_returns_p3Df = as.data.frame(all_returns_p3)
meanDf_p3 =  all_returns_p3Df %>% summarise_if(is.numeric, mean)
library(Hmisc)
hist.data.frame(all_returns_p3Df)
```

```{r 3c8, echo=FALSE, message=FALSE,warning=FALSE}
unlistedDf_p3 = data.frame(score = unlist(meanDf_p3))

rowList_p3 = rownames(unlistedDf_p3)
newDf_p3 = as.data.frame(cbind(rowList_p3,unlistedDf_p3))
ggplot(newDf_p3,
            aes(x=rowList_p3,y=score)) +
 geom_bar(stat="identity") + ggtitle("ETF Returns for Portfolio -3") +
 xlab("ETF") + ylab("ETF Return Score") 
```
* We will assign more weight to IJR, QQQ and Voa and less weight to Agg and UUP

```{r 3c9, echo=FALSE, message=FALSE,warning=FALSE}
 set.seed(1)
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth_p3s2 = initial_wealth
	weights = c(0.02,0.05,0.05,0.4,0.3,0.03,0.05,0.1)
	n_days = 20
	wealthtracker_p3s2 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_p3, 1, orig.ids=FALSE)
	  holdings = weights * total_wealth_p3s2
		holdings = holdings + holdings*return.today
		total_wealth_p3s2 = sum(holdings)
		wealthtracker_p3s2[today] = total_wealth_p3s2
	}
	wealthtracker_p3s2
}

hist(sim3[,n_days], 25,main = 'Histogram of returns for past 20 days for P3S2',xlab='Holdings')

```

```{r 3c10, echo=FALSE, message=FALSE,warning=FALSE}
plot(wealthtracker_p3s2, type='l',xlab='Days','ylab'='Total Wealth',main='Wealth change in 20 day window for P3S2')
```

```{r 3c11, echo=FALSE, message=FALSE,warning=FALSE}
meanReturn_p3s2 = mean(sim3[,n_days] - initial_wealth)
cat('Mean of profits for P3S2 is: ')
meanReturn_p3s2
```

```{r 3c12, echo=FALSE, message=FALSE,warning=FALSE}
# 5% value at risk:
var_p3_s2 = quantile(sim3[,n_days]- initial_wealth, prob=0.05)
cat("Var for 5% return for Portfolio - 3 and scenario 2 is : ")
var_p3_s2
```
* The mean return is still less even though Var is lesser compared to Portfolio-1. This is due to the fact that equity ETFs can be more profitable due to their volatile nature and the increased market capitalization


```{r 3c13, echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
profit_p3s2= sim3[,n_days]- initial_wealth
(hist(profit_p3s2,breaks = 30, main='Histogram of profits/Losses for P3S2',xlab="Profit"))
abline(v=var_p3_s2,col="red",lwd=3)
abline(v=mean(profit_p3s2),col="blue",lwd=3)
text(4000,600,"MeanLine")
text(-8500,700,"VAR Risk Line")
```



```{r 3c14, echo=FALSE, message=FALSE,warning=FALSE}
portfolios = c('P1S1','P1S2','P2S1','P2S2','P3S1','P3S2')
returns = c(meanReturn_p1,meanReturn_p1s2,meanReturn_p2,meanReturn_p2s2,meanReturn_p3s1,meanReturn_p3s2)
returnsDf = data.frame(Portfolio=portfolios,Returns=returns)

ggplot(returnsDf,aes(x=Portfolio,y=Returns)) + geom_bar(stat = "identity") + ggtitle('Mean Profit for Portfolios')
```
* To summarize the mean returns for Portfolio-1, scenario - 2 are desirable when we assign more weight to IJR and QQQ ETFs. Portfolio - 1 gives more returns since equity ETFs give higher returns compared to other asset class as there are higher risk involved as seen in the below graph.

```{r 3c15, echo=FALSE, message=FALSE,warning=FALSE}

vars = c(var_p1,var_p1s2,var_p2_1,var_p2_s2,var_p3_s1,var_p3_s2)
varDf = data.frame(Portfolio=portfolios,var_pct=vars)

ggplot(varDf,aes(x=Portfolio,y=var_pct)) + geom_bar(stat = "identity") + ggtitle('Var for Portfolios')
```
* Because bond ETFs never mature, they never offer the same protection for your initial investment the way that individual bonds can. In other words, you aren't guaranteed to get your money back at some point in the future. You can lose money if interest rates rise. Interest rates change over time. With so many incidents happening in the past 5 years like Covid, Russia-Ukraine War, inflation, it makes sense that with rising interest rates bonds give lesser returns as compared to ETFs.

**As they say, High Risk means High Rewards**


# Q6: Clustering & PCA
```{r 6a, echo=FALSE, message=FALSE,warning=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
```

Let's load in the data and take a look at it.

```{r 6b, echo=FALSE, message=FALSE,warning=FALSE}
wine <- read.csv('STA380/data/wine.csv')
head(wine)
```

This data set has 13 columns about different bottles of wine. The first 11 columns describe the chemical properties of the wine and the last two columns contain information about the color of the wine & its quality (rated by experts).

Our goal is to run unsupervised learning algorithms on the 11 chemical properties of the wines. Through the results of the unsupervised techniques, we will check if differences between the labels (red/white & quality) emerge naturally.

First, let's run PCA (Principal Component Analysis)

```{r 6c, echo=FALSE, message=FALSE,warning=FALSE}
PCAwine <- prcomp(wine[,-(12:13)], scale=TRUE)
summary(PCAwine)
# Let's take 2 components as these 2 alone help in explaining ~50% of variance
```

The third row of the summary table shows what is the cumulative proportion of variance that is captured through the corresponding principal components. In our case, we see that two principal components capture ~50% of the variance in the data set. So, let's take the number of principal components as two as this also helps us visualize the data easily.

Next, we'll interpret the loadings for each principal component, i.e., see how much weightage each component has given to our original 11 features.

```{r 6d, echo=FALSE, message=FALSE,warning=FALSE}
# create a tidy summary of the loadings
loadings_summary = PCAwine$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Properties')

loadings_summary %>%
  select(Properties, PC1) %>%
  arrange(desc(PC1)) %>%
  mutate(PC1 = round(PC1,2))
```

We can see that PC1 seems to give more positive loadings to sulfur dioxide and residual sugar while chlorides, sulphates & acidity have negative loadings.

```{r 6e, echo=FALSE, message=FALSE,warning=FALSE}
loadings_summary %>%
  select(Properties, PC2) %>%
  arrange(desc(PC2)) %>%
  mutate(PC2 = round(PC2,2))
```

PC2 gives more positive loading to density and high negative to alcohol content.

Now, let's plot our original data using these two principal components as the two axes and see if any pattern emerges.

```{r 6f, echo=FALSE, message=FALSE,warning=FALSE}
wine = merge(wine, PCAwine$x[,1:2], by="row.names")
wine = rename(wine, Show = Row.names)

wine %>% ggplot(aes(x = PC1, y = PC2)) + geom_point()
```

We can see that there are two clusters emerging. One to the left having more negative PC1 & another to the right having more positive PC1. There doesn't appear to be much difference along PC2. Using the interpretation of the feature loadings of PC1, we can see that wines having a high content of sulfur dioxides and residual sugars would have positive values while wines with high content of chlorides, sulphates and acidity would take negative values. Hence, we can say that these are two distinct clusters. However, we cannot say for certain which of these two clusters correspond to red & White wine without using the label information.

In the real world, we usually do not have access to this information. Nevertheless, for the purposes of validation, we can use the wine color to confirm if these two clusters do correspond to different types of wines.

```{r 6g, echo=FALSE, message=FALSE,warning=FALSE}
wine %>% ggplot(aes(x = PC1, y = PC2, col = color)) + geom_point()
```

We can clearly see that the principal components have clearly separated red & white wines. Hence, we can say that PCA is capable of distinguishing between red & white wines.

Next, we need to check if this model can distinguish between higher & lower quality wines. We continue to look at the original scatterplot using the principal components.

```{r 6h, echo=FALSE, message=FALSE,warning=FALSE}
wine %>% ggplot(aes(x = PC1, y = PC2)) + geom_point()
```

While we see two distinct clusters, it becomes harder to argue that these may correspond to higher & lower quality wines. This is because quality is a subjective metric. The data set describes quality as a rating on a 1-10 scale by a panel of wine experts. As a result, it may be possible that an expert's personal preference may have influenced their rating of the wine. Consequently, the actual chemical properties of the wine may not play an important role in the quality rating.

For checking if our argument is right, let's first bucket the quality score into 'High' & 'Low' and use this to color in the scatterplot. We will consider wines as high quality if they have a rating above 5.

```{r 6i, echo=FALSE, message=FALSE,warning=FALSE}
wine$Quality_score <- NA
wine[wine$quality <= 5,]$Quality_score <- 'Low'
wine[wine$quality > 5,]$Quality_score <- 'High'

wine %>% ggplot(aes(x = PC1, y = PC2, col = Quality_score)) + geom_point()
```

As we thought, the principal components do not do a very good job of separating high vs low quality wines. Thus, PCA cannot distinguish between wine quality.

Next, let's run k-Means clustering on the data. First, let's center and scale the data.

```{r 6j, echo=FALSE, message=FALSE,warning=FALSE}
wine <- read.csv('STA380/data/wine.csv')

X = wine[,-(12:14)]
X = scale(X, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

Our aim is to predict either Red/White or High/Low quality wine. Hence, we need k-Means to return two clusters.
```{r 6k, echo=FALSE, message=FALSE,warning=FALSE}
clust1 = kmeans(X, 2, nstart=25)
```

Let's take a look at the centers of the two clusters after bringing the units back to the original scale.

```{r 6l, echo=FALSE, message=FALSE,warning=FALSE}
print('Cluster 1:')
clust1$center[1,]*sigma + mu
```

```{r 6m, echo=FALSE, message=FALSE,warning=FALSE}
print('Cluster 2:')
clust1$center[2,]*sigma + mu
```

As it is a bit difficult to understand the differences between the two clusters due to the varying scales of the features, let's subtract the centers of the two clusters to get the difference.

```{r 6n, echo=FALSE, message=FALSE,warning=FALSE}
sort(clust1$center[1,] - clust1$center[2,])
```

We see that the cluster 1 center has more sulfur dioxide & residual sugar compared to cluster 2. Similarly, cluster 2 center has more acidity, chlorides & sulphates compared to cluster 1.

It's becoming clear that these two clusters are distinct as the main differentiating features are the same as the ones we saw from PCA.

In order to visualize, let's create a scatterplot using the two features with the highest magnitude of difference, i.e., Total Sulfur Dioxide & Volatile Acidity. We'll use the two clusters to color in the data points.

```{r 6o, echo=FALSE, message=FALSE,warning=FALSE}
wine %>% ggplot(aes(x = total.sulfur.dioxide, y = volatile.acidity, col = factor(clust1$cluster))) + geom_point()
```

We can see that these two clusters are distinctly separate from each other in terms of chemical characteristics. Again, we don't know which cluster corresponds to red vs white wine but we can be fairly confident that these two clusters denotes the wine colors.

Similar to PCA, k-Means clustering cannot do a good job of separating high quality & low quality wines for the reasons mentioned earlier.

Thus, we can conclude that k-Means clustering can distinguish between red and white wines.

# Q7: Market segmentation

```{r 7a, echo=FALSE, message=FALSE,warning=FALSE}
library(tidyverse)  # data manipulation
# library(cluster)    # clustering algorithms
# library(factoextra)
scl_data = read.csv('STA380/data/social_marketing.csv',na.strings = '')
str(scl_data)

#Removing the labels from the data
library(corrplot)
cormat <- cor(scl_data[c(2:37)])
corrplot(cormat,method = 'shade',type = 'lower')
```

**Findings:**       
* Shopping and photo-sharing are positively correlated     
* College_uni and online_gaming stands out with a strong positive correlation   
* Health_nutrition,peronal_fitness and outdoors have a high positive correlation showing these people are health conscious  
* Fashion and beauty have a strong postive correlation  

We can include all the variables in the cluster analysis to understand if the same points appear after profiling the clusters  

```{r 7b, echo=FALSE, message=FALSE,warning=FALSE}
market = read.csv("STA380/data/social_marketing.csv", row.names=1)
library(tidyr)
library(mosaic)
library(dplyr)
library(reshape2)
library(ggplot2)

et <- cbind(market[,2:4],market[6:36])
market_scale = market
cormat <- round(cor(market), 2)
melted_cormat <- melt(cormat)

market_scale = market/rowSums(market)
market_scale = scale(market_scale, center = TRUE, scale = FALSE)
pc3 = prcomp(market_scale, scale=FALSE, rank=3)
plot(pc3)

```

```{r 7c, echo=FALSE, message=FALSE,warning=FALSE}
loadings = pc3$rotation
scores = pc3$x
pairs(scores)
```



We observe the three categories 

* Fitness Enthusiast - These are part of the active community who look for outdoor activities, cooking, and health and nutrition. This community is important as they could be the majority consumers of NutrientH20 but they could also be very particular about everything and detail oriented. So, Nutrient H20 should make sure that this segment is always satisfied and doesn't have any consumer dissonance. 

* The Instagrammer/Vlogger -  These are very social people and let everyone know about everything they do. They are buying the brand because of their friend, influencer or because of the brand value. consistent advertising to keep the brand image should work on this group.

* The Young College Kids - They probably buy it from their vending machines or close convenience stores. The key to this segment is purely through distribution and college events. 



Step 2: Normalize the data and perform k - means clustering      

There is no considerable decrease in the error after 8 clusters.  
Hence 8 clusters were considered to be optimal for the analysis  

```{r 7d, echo=FALSE, message=FALSE,warning=FALSE}
# Standardizing the variables before clustering
scl_scaled <- scale(scl_data[,3:37], center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(scl_scaled,"scaled:center")
sigma = attr(scl_scaled,"scaled:scale")

# scree plot
set.seed(44)
#Plotting the elbow curve
k = 15
l = list()
for (i in 1:k){
  set.seed(4)
  l[[i]] <- kmeans(scl_scaled, i, nstart = 20 )$tot.withinss
}

plot(1:k, l,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

set.seed(4)
clust1 = kmeans(scl_scaled, 8, nstart=25)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

#Extracting the cluster centers

c1 = clust1$center[1,]*sigma + mu
c2 = clust1$center[2,]*sigma + mu
c3 = clust1$center[3,]*sigma + mu
c4 = clust1$center[4,]*sigma + mu
c5 = clust1$center[5,]*sigma + mu
c6 = clust1$center[6,]*sigma + mu
c7 = clust1$center[7,]*sigma + mu
c8 = clust1$center[8,]*sigma + mu

cluster_centers = rbind(c1,c2,c3,c4,c5,c6,c7,c8)
cluster_centers = t(cluster_centers)

# Visualizing the clusters on a dimensionally reduced plot
fviz_cluster(clust1, data = scl_scaled)
```
**The cluster separation is not very clear from the plot. So lets analyze the cluster centers to come up with the profiles based on the clusters**  


**Findings:**  
* There are multiple interesting profiles that came out of the clusters       

Cluster 2: People who are grouped under cluster 2 tweet a lot about photo sharing,cooking,beauty and fashion  

Cluster 3: This segment is profoundly university students as they tweet about sports,universities and online games  

Cluster 4: This segment of people are potentially health conscious people as they tweet mostly about health,nutrition,outdoors and personal fitness  

Cluster 5: Most of the tweets that these people tweet are adult related tweets 

Cluster 6: This segment of people are interested in tweeting about films, art, music, tv and dating  

Cluster 7: This segment of people might be profoundly adults as they tweet a lot about religion, parenting, family, sports, school,food and crafts

Cluster 8: People from this segment are interested in tweeting about travel, politics, news and automotive


# Q8: The Reuters corpus

```{r 8a, echo=FALSE, message=FALSE,warning=FALSE}

library(tm) 
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

authors_train = Sys.glob('STA380/data/ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL
author_names = substring(authors_train, first=23)
author_names=substr(author_names,4,nchar(author_names))


for(author in authors_train) {
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  author_name = substring(author, first=23)
  labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}
train = lapply(file_list_train, readerPlain) 
mynames_train = file_list_train %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist
# Rename the articles
names(train) = mynames_train
#names(train)
#labels_train
authors_test = Sys.glob('STA380/data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in authors_test) {
  files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add_test)
  author_name_test = substring(author, first=22)
  labels_test = append(labels_test, rep(author_name_test, length(files_to_add_test)))
}
test = lapply(file_list_test, readerPlain) 
mynames_test = file_list_test %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist
names(test) = mynames_test

```

2. Now, we will perform tokenization by removing whitespaces, sparse terms, stopwords, punctuations and make the letters lowercase. We perform the same steps on both the test and train files. 

3. We create a Document Term matrix 

```{r 8b, echo=FALSE, message=FALSE,warning=FALSE}
documents_raw = Corpus(VectorSource(train))
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


documents_raw_test = Corpus(VectorSource(test))
my_documents_test = documents_raw_test
my_documents_test = tm_map(my_documents_test, content_transformer(tolower)) # make everything lowercase
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers)) # remove numbers
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation)) # remove punctuation
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))

DTM_train = DocumentTermMatrix(my_documents)
DTM_test = DocumentTermMatrix(my_documents_test)
DTM_train <- removeSparseTerms(DTM_train, 0.91)
DTM_train_freq <- as.data.frame(as.matrix(DTM_train))
DTM_train_freq <- as.data.frame(as.matrix(DTM_train_freq))
DTM_test <- removeSparseTerms(DTM_test, 0.96)
DTM_test <- as.data.frame(as.matrix(DTM_test))

DTM_test_norm <- DTM_test[ ,(names(DTM_test) %in% names(DTM_train_freq))]
train = as.matrix(DTM_train_freq)
test =  as.matrix(DTM_test_norm)
train_Y =  as.factor(labels_train)
test_Y = as.factor(labels_test)
train_df = cbind(as.data.frame(train), as.data.frame(train_Y))

tdm <- TermDocumentMatrix(my_documents)
tdm <- as.matrix(tdm)
v<-sort(rowSums(tdm), decreasing=TRUE)
d<- data.frame(word=names(v), freq=v)
w <-rowSums(tdm)
w_sub <- subset(w, w>5000)
names(w_sub)
barplot(w_sub, col=rainbow(20), names.arg=names(w_sub),cex.names=0.7, las=3)

```


* The above are the most frequent words observed. Though we will not remove these words for now. In Inverse Term Frequency weighting, we could remove the word 'Said' as that may not be very relevant in our search.


Problem Statement: We try to predict the author by using the words in documents in the train folder. 

 We ran Naive Bayes to predict the author based on the word pattern count.
 
```{r 8c, echo=FALSE, message=FALSE,warning=FALSE}

library(naivebayes)
classifier <-  naive_bayes(train, train_Y, laplace = 1)
pred <- predict(classifier, test, type = 'class')
pred_tab <- table("Predictions"= pred,  "Actual" = labels_test)
NB_r <- sum(diag(pred_tab))/sum(pred_tab)
cat("The accuracy",NB_r )

```



```{r 8d, echo=FALSE, message=FALSE,warning=FALSE}
library(randomForest)
colnames(train_df) <- paste(colnames(train_df), "_c", sep = "")

test_rf <- test
colnames(test_rf) <- paste(colnames(test), "_c", sep = "")
model_rf <- randomForest(train_Y_c ~ . ,data = train_df, ntree=1250, mtry = 20) # Non cv
pred_rf <- predict(model_rf, test_rf, type = 'class')
pred_tab_rf <- table("Predictions"= pred_rf,  "Actual" = labels_test)
rf_r <- sum(diag(pred_tab_rf))/sum(pred_tab_rf)
cat("The accuracy for Random Forest", rf_r)

```


```{r 8e, echo=FALSE, message=FALSE,warning=FALSE}
accuracyDf <- data.frame(name=c("Naive Bayes","Random Forest"),value = c(32.76,56.16))
library(ggplot2)
plot<-ggplot(accuracyDf,
             aes(name,value)) +
  geom_bar(stat = "identity")+
  geom_text(aes(label = signif(value)), nudge_y = 3) + ggtitle("ClassifciModel Accuracy Score") +
  xlab("Models") + ylab("Accuracy Score") + theme(plot.title = element_text(color = "red")) + theme(plot.title = element_text(face = "italic")) + theme(axis.title.x = element_text(colour = "blue"),axis.title.y = element_text(colour = "red")) + theme(axis.title.x = element_text(face = "bold"),axis.title.y = element_text(face = "bold"))  
plot
```


Results: We observed that Naive Bayes accuracy is quite low and hence, we check with Random Forest. We achieved a higher accuracy but there are some concerns.
 Technically, with just a word count, predicting an author would be tough. What we could think about is predicting a genre the author is known for. If we do have a table to train the model with authors and their respective genres, I believe we could achieve more accuracy with such problem statements. 
 
Conclusions: This dataset is a goldmine to understand  many other factors.For eg, if we have the book revenue/popularity of such authors, we would be able to predict what keywords produce more revenue or are more popular. The applications of such a dataset could also be extended to songs, podcasts, etc. This could then be utilized to think of strategies to increase interest among public as well. 

# Q9: Association rule mining

```{r, echo=FALSE}
library(tidyverse)
library(igraph)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(tidyr)
library(igraph) 
```

Let's load in our data (groceries.txt). We see that the file consists of many rows with each row being a shopping basket. Unfortunately for us, this means that each row has different number of items and hence different number of columns. If we tried reading this file using the default function, R wouldn't read it correctly.

Hence, we first need to figure out what is the maximum number of columns present in the data set, i.e., maximum number of items in a shopping basket. R provides a function *count.fields* which can help us with this.
```{r,fig.align='left', echo=FALSE}
no_col <- max(count.fields("STA380/data/groceries.txt", sep = ","))
cat('Maximum number of columns in the file = ', no_col)
```

We see that the maximum number of columns is 32. Now that we know how many columns are there, we can read the file. We also need to tell R to not consider the first line as the header. Further, R needs to be told that the rows have unequal lengths so that it can fill the missing columns with blanks.
```{r, echo=FALSE}
groceries_raw = read.delim("STA380/data/groceries.txt",
                           sep = ',',header = FALSE, fill = TRUE,  col.names = paste0("V",seq_len(no_col)))
```

Let's look at the size of the data set.
```{r, echo= FALSE}
dim(groceries_raw)
```

The data set has ~10k shopping baskets. Now, let's do some preprocessing to convert this data set into a format that is expected by the 'arules' package.

```{r, echo=FALSE}
groceries_raw <- cbind(1:nrow(groceries_raw), groceries_raw)
colnames(groceries_raw)[1] <- 'Basket_no'
groceries <- pivot_longer(groceries_raw, cols = 2:33, values_to = 'items')
groceries$name <- NULL
groceries <- groceries[groceries$items != "",]
groceries$Basket_no = factor(groceries$Basket_no)
groceries = split(x=groceries$items, f=groceries$Basket_no)
groceries_trans = as(groceries, "transactions")
```

Before running the apriori algorithm, let's quickly check what is the proportion of the # of items present in shopping baskets. The below graph shows a running total of the % proportion for each # of items present in the basket.

```{r, echo=FALSE}
summ_groc <- summary(groceries_trans)
rule_lengths <- as.data.frame(round(cumsum(summ_groc@lengths)/max(cumsum(summ_groc@lengths))*100,2))
colnames(rule_lengths)<- 'Cumulative Sum'

rule_lengths %>% ggplot(aes(x = as.integer(row.names(rule_lengths)), y = `Cumulative Sum`)) +
  geom_point() + 
  geom_line() + 
  xlab('# of items in basket') +
  ylab('% Running total of baskets ')
```

We see ~75% of the shopping baskets have # of items <= 6. Therefore, we'll use 6 items as the maximum item length in the apriori algorithm. We'll also only look at item sets with support more than 0.005 (present in >50 baskets in our data set) and confidence more than 0.05. This will ensure we get discover rules backed by a significant number of shopping baskets.

```{r, echo=FALSE, results='hide'}
grocery_rules = apriori(groceries_trans, 
                     parameter=list(support=.005, confidence=.05, maxlen=6))
```

Let's look at 10 rules for the baskets where lift > 3. This means that we want to look at rules where the presence of the LHS items in a basket will increase the probability of buying the RHS item by a factor of 3.

```{r, echo=FALSE}
inspect(head(sort(subset(grocery_rules, subset=lift > 3), by = 'lift', decreasing = TRUE),10))
```

The rules with the highest lift relate to ham & white bread. This makes sense. Most people would buy these two items to make a ham sandwich but usually not each item individually. The next few rules relate to fruits and vegetables. The last two rules are about cream and berries. These two items would usually not be purchased a lot individually. However, most people would buy both of these while making desserts, cakes, etc. which explains why it has a high lift.

Next, we'll look at 10 rules where the confidence is more than 0.5, i.e., the probability of the RHS item being bought given that the LHS items are present in the basket.

```{r, echo=FALSE}
inspect(head(sort(subset(grocery_rules, subset=confidence > 0.5), by = 'confidence', decreasing = TRUE),10))
```

We see that most of these rules involve Whole milk as the RHS item. Therefore, the confidence is interpreted as what is the probability of buying whole milk given one has bought the LHS items. For example, if one has already bought root vegetables, tropical fruit and yogurt, the probability of them buying whole milk is 0.7. 

However, these rules have lower lifts compared to the rules seen earlier. Why is that? This is because milk is something that is bought very frequently ,i.e., it has a high probability of being bought. Hence, the increase in probability that milk will be bought given the other items were bought would be lesser.

Finally, let's visualize the network with all rules with lift > 3.

```{r, echo=FALSE, fig.height = 15, fig.width = 20}
groceries_graph = associations2igraph(subset(grocery_rules, lift>3), associationsAsNodes = FALSE)     

V(groceries_graph)$color = "orange"
V(groceries_graph)$frame.color = 0
V(groceries_graph)$label.color = "black"
V(groceries_graph)$label.cex = 1.4
V(groceries_graph)$size = 10
V(groceries_graph)$arrow.mode = 0
par(mar=c(0,0,0,0))
plot(groceries_graph, edge.curved=FALSE, edge.arrow.size = 0.4 )

```

There are a lot of associations between commonly bought items like milk, vegetables, butter and fruits. We also see some other isolated associations which make sense. For example, napkins & hygiene articles, waffles & chocolate, ham & bread. Thus, if a grocery store should choose to act on this data, they should keep these items where the lift is > 3 close to each other.